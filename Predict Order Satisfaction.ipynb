{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Creating SQL View for cat_names ---\n",
      "--- Creating SQL View for classified_orders ---\n",
      "--- Creating SQL View for orders_general ---\n",
      "--- Creating SQL View for payments ---\n",
      "--- Creating SQL View for products ---\n",
      "--- Creating SQL View for public_customers ---\n",
      "--- Creating SQL View for sellers ---\n",
      "--- Creating SQL View for geo_table ---\n",
      "--- Creating SQL View for orders_distance ---\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from pyspark.sql import SQLContext\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler\n",
    "pd.options.display.max_columns = 999\n",
    "try:\n",
    "    sc and spark\n",
    "except (NameError, UnboundLocalError) as e:\n",
    "\n",
    "    import findspark\n",
    "\n",
    "    findspark.init()\n",
    "    import pyspark\n",
    "    import pyspark.sql\n",
    "\n",
    "    sc = pyspark.SparkContext()\n",
    "    spark = pyspark.sql.SparkSession(sc).builder.appName(\n",
    "        \"E-Commerce Analytics\").getOrCreate()\n",
    "    sqlContext = SQLContext(spark)\n",
    "\n",
    "\n",
    "def spark_read_parquet(path):\n",
    "   return sqlContext.read.parquet(path)\n",
    "\n",
    "\n",
    "base = \"/Users/davidkatzaudio/Desktop/bi_ecom_project/serialized_data/\"\n",
    "\n",
    "cat_names = spark_read_parquet(base+\"cat_names.parquet\")\n",
    "classified_orders = spark_read_parquet(base+\"classified_orders.parquet\")\n",
    "orders_general = spark_read_parquet(base+\"orders_general.parquet\")\n",
    "payments = spark_read_parquet(base+\"payments.parquet\")\n",
    "products = spark_read_parquet(base+\"products.parquet\")\n",
    "public_customers = spark_read_parquet(base+\"public_customers.parquet\")\n",
    "sellers = spark_read_parquet(base+\"sellers.parquet\")\n",
    "geo_table = spark_read_parquet(base+\"geo_table.parquet\")\n",
    "orders_distance = spark_read_parquet(base+\"orders_distance.parquet\")\n",
    "\n",
    "dataframes = {\n",
    "\n",
    "    \"cat_names\": cat_names,\n",
    "    \"classified_orders\": classified_orders,\n",
    "    \"orders_general\": orders_general,\n",
    "    \"payments\": payments,\n",
    "    \"products\": products,\n",
    "    \"public_customers\": public_customers,\n",
    "    \"sellers\": sellers,\n",
    "    \"geo_table\": geo_table,\n",
    "    \"orders_distance\": orders_distance,\n",
    "}\n",
    "\n",
    "for item in dataframes.items():\n",
    "    print(\"--- Creating SQL View for %s ---\" % item[0])\n",
    "    item[1].createOrReplaceTempView(item[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\"SELECT * FROM classified_orders\").toPandas().set_index(\"index_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df[\"customer_city\"] = df[\"customer_city\"].str.strip().str.lower()\n",
    "df[\"approval_delay\"] = (df[\"order_aproved_at\"] - df.order_purchase_timestamp).apply(lambda x: x.seconds)\n",
    "df[\"order_month\"] = df.order_purchase_timestamp.apply(lambda x:x.month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "\n",
    "    \"order_freight_value\",\n",
    "    \"order_items_qty\",\n",
    "    \"product_photos_qty\",\n",
    "    \"order_products_value\",\n",
    "    \"product_category_name\",\n",
    "    \"approval_delay\",\n",
    "    \n",
    "] \n",
    "\n",
    "categoricals = [\"customer_state\", \"product_category_name\", \"order_month\"]\n",
    "\n",
    "features = [i for i in features if i not in categoricals]\n",
    "target = [\"most_voted_class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3303, 35)"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = OneHotEncoder()\n",
    "normalizer = StandardScaler()\n",
    "X_conts = pd.DataFrame(normalizer.fit_transform(X = df[features].reset_index(drop=True)), columns=features)\n",
    "\n",
    "cats_df = pd.DataFrame(encoder.fit_transform(df[categoricals]).todense(), columns=encoder.get_feature_names())\n",
    "X = pd.concat([X_conts, cats_df], axis=1)\n",
    "feature_names = X.columns\n",
    "y = (df[target]\n",
    "     .replace(\"satisfeito_com_pedido\", 0)\n",
    "     .replace(\"problemas_de_entrega\", 1)\n",
    "     .replace(\"problemas_de_qualidade\", 1).\n",
    "     values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Score: 0.667339\n",
      "Mathew Corr Coeff: 0.280632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidkatzaudio/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n",
    "model = RandomForestClassifier(n_estimators=30,max_depth=105, class_weight=\"balanced_subsample\",criterion=\"entropy\", )\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Model Score: %3f\" % model.score(X_test,y_test))\n",
    "print(\"Mathew Corr Coeff: %3f\" % matthews_corrcoef(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
